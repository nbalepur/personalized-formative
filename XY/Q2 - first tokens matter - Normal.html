<head>    <style>.highlight-buttercream { background-color: #ffe066; padding: 2px 4px; border-radius: 4px; } .highlight-apricot { background-color: #ff9966; padding: 2px 4px; border-radius: 4px; } .highlight-mistgreen { background-color: #6fdcbf; padding: 2px 4px; border-radius: 4px; } .highlight-lavender { background-color: #d3a4f9; padding: 2px 4px; border-radius: 4px; } .highlight-powderblue { background-color: #9ecbfa; padding: 2px 4px; border-radius: 4px; }      body {        font-family: "Segoe UI", "Helvetica Neue", sans-serif;        background: #ffffff;        color: #06262d;        margin: 40px auto;        max-width: 70%;        line-height: 1.65;        font-size: 16px;      }        h2, h3 {        border-bottom: 2px solid #e0e0e0;        padding-bottom: 6px;        margin-top: 40px;        color: #06262d;      }        ul {        background: #f5eee6;        border-left: 4px solid #34b88e;        padding: 10px 20px;        margin-bottom: 20px;        list-style-type: none;      }        li::marker {        color: #ed5298;      }        b {        color: #ed5298;      }        p {        margin-bottom: 20px;      }        sub {        vertical-align: sub;        font-size: smaller;      }        ol {        font-size: 0.95em;        padding-left: 20px;      }        ol li {        margin-bottom: 6px;      }        a {        color: #2980b9;        text-decoration: none;      }        a:hover {        text-decoration: underline;      }    </style>  </head><b>Query:</b> why first few tokens matter in llm safety<h2>Section 1: Shallow Safety Alignment and Its Impact</h2>
<ul> <li> <b>TL;DR:</b> Current safety measures in Large Language Models (LLMs) often result in "shallow safety alignment," where the model's decision to refuse or comply with a prompt is heavily determined by the first few tokens it generates. This occurs because alignment training often teaches models to start refusal responses with specific phrases.</li> </ul>
<p>The first few tokens generated by a Large Language Model (LLM) are critical for safety due to a phenomenon known as "shallow safety alignment" [<a href="#ref-1">1</a>, <a href="#ref-3">3</a>]. This means that the safety training an LLM undergoes primarily adapts the model's generative distribution over only its very first few output tokens to produce a refusal response [<a href="#ref-1">1</a>, <a href="#ref-3">3</a>]. Consequently, if these initial tokens deviate from standard safe refusal prefixes (e.g., "I cannot," "I apologize"), the model is much more likely to generate harmful or undesirable content, even if the prompt is malicious [<a href="#ref-3">3</a>, <a href="#ref-4">4</a>, <a href="#ref-16">16</a>]. This is because the model's safety mechanisms are predominantly concentrated on this initial part of the response [<a href="#ref-3">3</a>, <a href="#ref-10">10</a>].</p>
<p>This shallow alignment stems from the characteristics of the data used for safety training, where human labelers are more likely to begin refusal sentences at the start rather than in the middle [<a href="#ref-1">1</a>, <a href="#ref-4">4</a>]. As a result, LLMs learn to associate harmful queries with specific refusal tokens at the beginning of a response [<a href="#ref-4">4</a>, <a href="#ref-6">6</a>]. This reliance on initial tokens means that the decision to comply with or refuse a request is often made within a very limited number of generation steps [<a href="#ref-2">2</a>]. The log-perplexity (a measure of how surprising a token is) is highest on the first token and drops significantly after, indicating an increased likelihood of harmful responses once the initial few tokens are generated, if those tokens are not refusal-oriented [<a href="#ref-2">2</a>].</p>

<h2>Section 2: Vulnerabilities and Defense Strategies Related to Initial Tokens</h2>
<ul> <li> <b>TL;DR:</b> The reliance on initial tokens for safety creates vulnerabilities, as attacks can target these first few tokens to bypass safety measures. Conversely, many defense strategies also focus on manipulating or protecting these initial tokens to ensure safe outputs.</li> </ul>
<p>Shallow safety alignment makes LLMs vulnerable to various attacks that specifically target these initial tokens. For example, attacks like GCG and prefilling attacks can manipulate the LLM into generating a non-refusal prefix, leading to harmful outputs [<a href="#ref-1">1</a>, <a href="#ref-3">3</a>]. Fine-tuning, even with benign data, can also inadvertently disrupt safety by altering the generative distribution of these crucial initial tokens, causing the model to "forget" its safety training for refusal prefixes [<a href="#ref-3">3</a>]. Studies show that fine-tuning on samples with very short answers (which heavily impact initial token distributions) can significantly degrade an LLM's safety alignment [<a href="#ref-9">9</a>]. Once forced to generate affirmative prefixes, models often exhibit a cascading failure, where subsequent tokens increasingly favor harmful content completion [<a href="#ref-10">10</a>].</p>
<p>On the other hand, understanding the importance of initial tokens has also guided the development of defense mechanisms. Many strategies aim to protect or control the generation of the first few tokens [<a href="#ref-1">1</a>, <a href="#ref-6">6</a>, <a href="#ref-7">7</a>]. For instance, "safety trigger tokens" are defined as the first few generated tokens that induce a refusal response when processing malicious prompts [<a href="#ref-1">1</a>]. Defense algorithms like D-STT identify these safety trigger tokens (often a single token) and explicitly decode them as a prefix to the response, thereby activating the model's learned safety patterns with minimal intervention [<a href="#ref-1">1</a>]. Other methods adjust the probability distribution of tokens during the initial decoding steps to steer the response towards safety [<a href="#ref-6">6</a>] or use classifiers to ensure the safety of each token generated, starting from the very first one [<a href="#ref-13">13</a>]. These approaches acknowledge that identifying unsafe output can often occur within the early stages of generation, such as the initial 25% of the content [<a href="#ref-12">12</a>].</p>

<h2>Section 3: Deeper Mechanisms and Broader Implications</h2>
<ul> <li> <b>TL;DR:</b> The focus on initial tokens is linked to how LLMs process information, with early layers discriminating harmfulness and alignment associating these assessments with initial refusal tokens. This highlights the need for safety measures that persist beyond the first few tokens.</li> </ul>
<p>The significance of the first few tokens is tied to the internal workings of LLMs. Research suggests that LLMs can identify malicious inputs in their early layers based on ethical concepts learned during pre-training [<a href="#ref-14">14</a>, <a href="#ref-19">19</a>]. Safety alignment then works by associating these early-layer assessments with "shallow guess tokens" (representing positive or negative emotions) in the middle layers, which are subsequently refined into specific affirmative or refusal initial response tokens [<a href="#ref-14">14</a>]. Jailbreak attacks often succeed by disturbing this association in the middle layers, preventing the transformation of an unethical classification into a refusal [<a href="#ref-14">14</a>, <a href="#ref-21">21</a>]. The initial tokens effectively set the tone for the entire response [<a href="#ref-6">6</a>], and their distribution is heavily influenced by alignment tuning compared to later tokens [<a href="#ref-5">5</a>, <a href="#ref-18">18</a>].</p>
<p>This heavy reliance on initial tokens for safety implies that current alignment methods may not be consistently applied throughout longer sequences, with the effect of alignment diminishing for later tokens [<a href="#ref-11">11</a>, <a href="#ref-17">17</a>, <a href="#ref-18">18</a>]. This can lead to situations where a model might start with a refusal but could still be steered towards generating harmful content later in the response if the initial safety signal is not robust enough or if attacks target deeper parts of the generation [<a href="#ref-1">1</a>, <a href="#ref-8">8</a>]. Therefore, there's a growing recognition that future safety alignment needs to be "more than just a few tokens deep," aiming for a "deep safety alignment" where models can recover from harmful starting conditions or maintain safety awareness throughout the entire generation process [<a href="#ref-3">3</a>, <a href="#ref-10">10</a>, <a href="#ref-17">17</a>].</p>

<h3>References</h3>
<ol><li id="ref-1">One Trigger Token Is Enough: A Defense Strategy for Balancing Safety and Usability in Large Language Models (Gu et. al., 2025)</li>
<li id="ref-2">Align in Depth: Defending Jailbreak Attacks via Progressive Answer Detoxification (Zhang et. al., 2025)</li>
<li id="ref-3">Safety Alignment Should Be Made More Than Just a Few Tokens Deep (Qi et. al., 2024)</li>
<li id="ref-4">Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training (Yuan et. al., 2024)</li>
<li id="ref-5">Improving LLM Safety Alignment with Dual-Objective Optimization (Zhao et. al., 2025)</li>
<li id="ref-6">LightDefense: A Lightweight Uncertainty-Driven Defense against Jailbreaks via Shifted Token Distribution (Yang et. al., 2025)</li>
<li id="ref-7">MoGU: A Framework for Enhancing Safety of Open-Sourced LLMs While Preserving Their Usability (Du et. al., 2024)</li>
<li id="ref-8">Probing the Safety Response Boundary of Large Language Models via Unsafe Decoding Path Generation (Wang et. al., 2024)</li>
<li id="ref-9">Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely Breaks Safety (Guan et. al., 2025)</li>
<li id="ref-10">Output Constraints as Attack Surface: Exploiting Structured Generation to Bypass LLM Safety Mechanisms (Zhang et. al., 2025)</li>
<li id="ref-11">LookAhead Tuning: Safer Language Models via Partial Answer Previews (Liu et. al., 2025)</li>
<li id="ref-12">Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path Forward (Xie et. al., 2024)</li>
<li id="ref-13">Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level (Zeng et. al., 2024)</li>
<li id="ref-14">How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States (Zhou et. al., 2024)</li>
<li id="ref-15">Multilingual Blending: LLM Safety Alignment Evaluation with Language Mixture (Song et. al., 2024)</li>
<li id="ref-16">AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs (Liao et. al., 2024)</li>
<li id="ref-17">Weak-to-Strong Jailbreaking on Large Language Models (Zhao et. al., 2024)</li>
<li id="ref-18">The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning (Lin et. al., 2023)</li>
<li id="ref-19">Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing (Zhao et. al., 2024)</li>
<li id="ref-20">Locking Down the Finetuned LLMs Safety (Zhu et. al., 2024)</li>
<li id="ref-21">Quantized Delta Weight Is Safety Keeper (Liu et. al., 2024)</li>
<li id="ref-22">KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization (Hooper et. al., 2024)</li>
<li id="ref-23">SaRO: Enhancing LLM Safety through Reasoning-based Alignment (Mou et. al., 2025)</li></ol>
