<head>    <style>.highlight-buttercream { background-color: #ffe066; padding: 2px 4px; border-radius: 4px; } .highlight-apricot { background-color: #ff9966; padding: 2px 4px; border-radius: 4px; } .highlight-mistgreen { background-color: #6fdcbf; padding: 2px 4px; border-radius: 4px; } .highlight-lavender { background-color: #d3a4f9; padding: 2px 4px; border-radius: 4px; } .highlight-powderblue { background-color: #9ecbfa; padding: 2px 4px; border-radius: 4px; }      body {        font-family: "Segoe UI", "Helvetica Neue", sans-serif;        background: #ffffff;        color: #06262d;        margin: 40px auto;        max-width: 70%;        line-height: 1.65;        font-size: 16px;      }        h2, h3 {        border-bottom: 2px solid #e0e0e0;        padding-bottom: 6px;        margin-top: 40px;        color: #06262d;      }        ul {        background: #f5eee6;        border-left: 4px solid #34b88e;        padding: 10px 20px;        margin-bottom: 20px;        list-style-type: none;      }        li::marker {        color: #ed5298;      }        b {        color: #ed5298;      }        p {        margin-bottom: 20px;      }        sub {        vertical-align: sub;        font-size: smaller;      }        ol {        font-size: 0.95em;        padding-left: 20px;      }        ol li {        margin-bottom: 6px;      }        a {        color: #2980b9;        text-decoration: none;      }        a:hover {        text-decoration: underline;      }    </style>  </head><b>Query:</b> How to identify explainable dimensions that differentiate a set of text, e.g. topic, language features, etc. I know PCA, but the resulting dimensions are not explainable and might not have semantic meanings
 <br /><h2>Section 1: Background: The Challenge of Interpretable Text Dimensions</h2>
<ul> <li> <b>TL;DR:</b> Standard methods like Principal Component Analysis (PCA) are effective for reducing the number of dimensions in text data, but the resulting components often lack clear semantic meaning. To differentiate texts in an explainable way, alternative techniques are needed to generate dimensions that correspond to human-understandable concepts like topics or stylistic features. </ul>

<p>A collection of texts can vary along several parameters, including topic, genre, and writing style [<a href="#ref-1">1</a>]. A common first step in analyzing text is to convert it into a numerical format, often a high-dimensional and sparse document-term matrix using models like bag-of-words [<a href="#ref-13">13</a>, <a href="#ref-26">26</a>]. Due to the "curse of dimensionality," where an excessive number of features can hinder analysis, dimensionality reduction is a necessary step [<a href="#ref-21">21</a>, <a href="#ref-22">22</a>]. However, a major challenge with common techniques like PCA is that they were not specifically designed for sparse, discrete text data [<a href="#ref-20">20</a>]. While PCA can reduce dimensionality, it is rotationally invariant, meaning the resulting components are linear combinations of the original features that often lack any direct, intuitive, or semantic interpretation [<a href="#ref-3">3</a>, <a href="#ref-9">9</a>, <a href="#ref-24">24</a>]. The goal for many researchers is to move beyond this and find methods that can partition texts along a small set of dimensions that are easily explainable [<a href="#ref-1">1</a>].</p>
<p>While some practices, such as applying varimax rotation, attempt to make PCA components more distinct by ensuring they are statistically independent, this does not guarantee that the resulting "themes" will be semantically coherent [<a href="#ref-30">30</a>]. The core issue is that many dimensionality reduction methods ignore the textual nature of the data, treating words as abstract, unrelated dimensions [<a href="#ref-24">24</a>]. To overcome this, researchers have developed methods that aim to produce semantically richer text representations. These techniques seek to discover deeper patterns by accounting for word relationships and the fact that different words can be used to express the same idea (synonymy) or the same word can have different meanings (polysemy) [<a href="#ref-13">13</a>, <a href="#ref-29">29</a>]. The objective is to identify dimensions that are not just mathematical constructs but correspond to interpretable concepts [<a href="#ref-3">3</a>, <a href="#ref-8">8</a>].</p>

<h2>Section 2: Topic Modeling for Semantic Topic Discovery</h2>
<ul> <li> <b>TL;DR:</b> Topic modeling techniques like Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA) analyze word co-occurrence patterns to uncover latent "topics" within a corpus. These topics serve as interpretable, low-dimensional representations of the main themes present in the documents. </ul>

<p>Topic modeling is a popular unsupervised approach for discovering the primary themes in a text collection [<a href="#ref-4">4</a>, <a href="#ref-13">13</a>]. These methods overcome the limitations of simple word-frequency models by creating low-dimensional representations where documents are described by the topics they contain [<a href="#ref-13">13</a>, <a href="#ref-26">26</a>]. Two foundational topic modeling techniques are Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA) [<a href="#ref-18">18</a>, <a href="#ref-26">26</a>].</p>
<p>LSA, also known as Latent Semantic Indexing (LSI), uses a matrix factorization technique called Singular Value Decomposition (SVD) on a document-term matrix [<a href="#ref-6">6</a>, <a href="#ref-17">17</a>, <a href="#ref-28">28</a>]. This process reduces the high-dimensional matrix to a smaller number of latent semantic dimensions, or topics [<a href="#ref-6">6</a>, <a href="#ref-17">17</a>]. LSA operates on the assumption that words with similar meanings will appear in similar contexts [<a href="#ref-14">14</a>, <a href="#ref-28">28</a>]. In contrast, LDA is a probabilistic model that assumes each document is a mixture of various topics, and each topic is a mixture of words [<a href="#ref-4">4</a>, <a href="#ref-20">20</a>]. LDA is often effective at handling synonymy and polysemy [<a href="#ref-29">29</a>], and its use of statistical priors helps create sparse and more easily interpretable topics [<a href="#ref-20">20</a>]. To ensure the quality and interpretability of these topics, coherence metrics can be used to help determine the optimal number of topics for a given corpus [<a href="#ref-4">4</a>].</p>

<h2>Section 3: Alternative Approaches for Explainable Dimensions</h2>
<ul> <li> <b>TL;DR:</b> Beyond general topic modeling, other methods can identify interpretable dimensions related to style, ideology, or predefined concepts. These include analyzing linguistic features, using advanced matrix factorization techniques, and applying supervision to align dimensions with specific meanings. </ul>

<p>Several methods exist for discovering explainable dimensions that go beyond the thematic topics found by LSA or LDA. These approaches can focus on different aspects of the text, such as style, or allow for more explicit control over the dimensions being created.
<ul>
    <li><b>Stylistic and Structural Analysis</b>: Multidimensional Analysis (MDA) is a statistical method used to identify dimensions of stylistic variation. It works by analyzing patterns of linguistic co-occurrence, such as the frequent use of nouns and prepositions in formal texts versus pronouns and interjections in informal ones [<a href="#ref-2">2</a>]. Another approach uses spectral clustering to induce multiple "clustering dimensions" from a dataset, which can correspond not only to topics but also to non-topic attributes like sentiment or gender [<a href="#ref-25">25</a>].</li>
    <li><b>Interpretable Matrix Factorization</b>: To address the lack of interpretability in methods like PCA, researchers have developed alternative factorization techniques. Non-negative Matrix Factorization (NMF) has been shown to be effective for sparse, non-negative text data [<a href="#ref-22">22</a>, <a href="#ref-26">26</a>]. Other methods like Decomposition into Directional Components (DEDICOM) or the CoVeR algorithm are explicitly designed to break the rotational symmetry of traditional embeddings, yielding latent factors that align with interpretable topics [<a href="#ref-3">3</a>, <a href="#ref-8">8</a>]. Independent Component Analysis (ICA) has also been proposed as an alternative to PCA for identifying distinctive and interpretable semantic axes from word embeddings [<a href="#ref-16">16</a>].</li>
    <li><b>Supervised and Knowledge-Based Methods</b>: It is also possible to explicitly define the dimensions of interest. "Semantic Scaling" uses large language models to score documents based on user-defined ideological dimensions [<a href="#ref-10">10</a>]. Other supervised techniques can learn "supervised dimensions" by training a model to find projections in the embedding space that align with specific concepts, which can help disambiguate word senses [<a href="#ref-23">23</a>]. Similarly, by providing a model like CLIP with varying text prompts (e.g., "a red car," "a blue car"), one can identify the embedding dimensions that correspond to that specific variation (e.g., color) [<a href="#ref-31">31</a>]. Finally, domain knowledge can be integrated by using ontologies to guide feature selection or by defining a custom distance metric between words that encodes semantic similarity [<a href="#ref-21">21</a>, <a href="#ref-24">24</a>].</li>
</ul>
</p>

<h3>References</h3>
<ol><li id="ref-1">Recognizing Text Genres With Simple Metrics Using Discriminant Analysis (Karlgren et. al., 1994)</li>
<li id="ref-2">Stylistic variation on the Donald Trump Twitter account: A linguistic analysis of tweets posted between 2009 and 2018 (Clarke et. al., 2019)</li>
<li id="ref-3">CoVeR: Learning Covariate-Specific Vector Representations with Tensor Decompositions (Tian et. al., 2018)</li>
<li id="ref-4">Understanding service quality concerns from public discourse in Indonesia state electric company (Aditya et. al., 2023)</li>
<li id="ref-5">RELIANCE: Reliable Ensemble Learning for Information and News Credibility Evaluation (Ramezani et. al., 2024)</li>
<li id="ref-6">Who Needs External References?—Text Summarization Evaluation Using Original Documents (Foysal et. al., 2023)</li>
<li id="ref-7">Semantic Analysis for Automated Evaluation of the Potential Impact of Research Articles (Suzen et. al., 2021)</li>
<li id="ref-8">Interpretable Topic Extraction and Word Embedding Learning Using Non-Negative Tensor DEDICOM (Hillebrand et. al., 2021)</li>
<li id="ref-9">Causal Feature Selection with Dimension Reduction for Interpretable Text Classification (Shan et. al., 2020)</li>
<li id="ref-10">Semantic Scaling: Bayesian Ideal Point Estimates with Large Language Models (Burnham et. al., 2024)</li>
<li id="ref-11">An improved Arabic text classification method using word embedding (Sabri et. al., 2024)</li>
<li id="ref-12">The Research Trends of Text Classification Studies (2000–2020): A Bibliometric Analysis (Zhu et. al., 2022)</li>
<li id="ref-13">Knowledge-enhanced document embeddings for text classification (Sinoara et. al., 2019)</li>
<li id="ref-14">Approach for Multi-Label Text Data Class Verification and Adjustment Based on Self-Organizing Map and Latent Semantic Analysis (Stefanovič et. al., 2022)</li>
<li id="ref-15">Integrating topic modeling and word embedding to characterize violent deaths (Arseniev-Koehler et. al., 2020)</li>
<li id="ref-16">Discovering Universal Geometry in Embeddings with ICA (Yamagiwa et. al., 2023)</li>
<li id="ref-17">Do people communicate about their whereabouts? Investigating the relation between user-generated text messages and Foursquare check-in places (Li et. al., 2018)</li>
<li id="ref-18">A Comparison of Different Approaches to Document Representation in Turkish Language (Yıldırım et. al., 2018)</li>
<li id="ref-19">A Two-Sample Test of Text Generation Similarity (Xu et. al., 2025)</li>
<li id="ref-20">Unsupervised learning for medical data: A review of probabilistic factorization methods (Neijzen et. al., 2023)</li>
<li id="ref-21">Ontology Driven Feature Engineering for Opinion Mining (Siddiqui et. al., 2019)</li>
<li id="ref-22">Improving problem identification via automated log clustering using dimensionality reduction (Rosenberg et. al., 2018)</li>
<li id="ref-23">Supervised Understanding of Word Embeddings (Yerebakan et. al., 2020)</li>
<li id="ref-24">Linguistic Geometries for Unsupervised Dimensionality Reduction (Mao et. al., 2010)</li>
<li id="ref-25">Which Clustering Do You Want? Inducing Your Ideal Clustering with Minimal Feedback (Dasgupta et. al., 2010)</li>
<li id="ref-26">Topic modeling revisited: New evidence on algorithm performance and quality metrics (Rüdiger et. al., 2022)</li>
<li id="ref-27">PROBABILISTIC TOPIC MODELING AND ITS VARIANTS – A SURVEY (PadmajaChV et. al., 2018)</li>
<li id="ref-28">A Comparative Study of NLP Topic Modeling Methods and Tools (Goel et. al., 2019)</li>
<li id="ref-29">Defining and Evaluating Classification Algorithm for High-Dimensional Data Based on Latent Topics (Luo et. al., 2014)</li>
<li id="ref-30">The Meaning Extraction Method: An Approach to Evaluate Content Patterns From Large-Scale Language Data (Markowitz et. al., 2021)</li>
<li id="ref-31">InDiReCT: Language-Guided Zero-Shot Deep Metric Learning for Images (Kobs et. al., 2022)</li></ol>